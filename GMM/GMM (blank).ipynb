{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9393956",
   "metadata": {},
   "source": [
    "# Introduction to GMM\n",
    "\n",
    "## What is this and why am I learning it?\n",
    "GMM is a way of uncovering structural parameters from data that may or may not be linear. While we have seen over and over again how regressions can uncover unknown \"betas\" that summarize causal (and correlational) relationships between variables, these \"betas\" are _linear_: $y$ is a linear function of $\\beta$. However, in many of the models we may right down (especially in Trade, Economic Geography, IO...) we will have structural relationships that are not at all linear in the unknown parameters we want to estimate! \n",
    "\n",
    "To give a real-world example, take the system of structural equations from the model in Redding and Sturn (2008) (an Economic Geography, Rosen-Roback style model):\n",
    "\n",
    "$$w_i^{\\sigma} = \\sum_n L_nw_n (\\tau_{ni}/(P_nT_n))^{1-\\sigma} = \"FMA_i\"$$ \n",
    "\n",
    "where $P_n$ is the price index,  $w_i$ are wages in city $i$, $L_i$ is labor (population), $\\tau_{ni}$ are trade costs from $n$ to $i$, and $T_i$ are unknown productivity parameters. \n",
    "\n",
    "Suppose for simplicity that we know $P_n$ (the price index), we have data on $\\{w_n, L_n, \\tau_{ni}\\}_{i,n}$, and we are using a value for $\\sigma = 4$ from previous estimates. Then we can use this equation as a GMM moment condition to estimate $T_n$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaca63b",
   "metadata": {},
   "source": [
    "## A more simple example\n",
    "\n",
    "Let's walk through a more stylized example for our purposes. Suppose you have written down an IO model that goes like this:\n",
    "\n",
    "Firms produce 2 goods $g = 1,2$ using the input $x$ for each. However, each good uses a different technology, like so:\n",
    "\n",
    "$$y_g = f_g(x)$$\n",
    "\n",
    "This is essentially what we saw in class. Now to be more explicit, we are going to say that our model assumes Cobb-Douglass technology, so that our production functions look like this:\n",
    "\n",
    "\\begin{align}\n",
    "y_1 = x_1^{\\alpha_1} \\\\ \n",
    "y_2 = x_2^{\\alpha_2} \n",
    "\\end{align}\n",
    "\n",
    "$x_1, x_2$ index how much of $x$ is used in the production of goods 1, 2 respectively.\n",
    "\n",
    "Now firms have uncertainty over their prices. Then our first order conditions from the model imply that the following should hold:\n",
    "\\begin{align}\n",
    "E\\left(p_1\\alpha_1x_1^{\\alpha_1 - 1} - w\\right) &= 0 \\\\ \n",
    "E\\left(p_2\\alpha_2x_2^{\\alpha_2-1} - w\\right) &= 0 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Now, we have data on lots of firms $f = 1,\\ldots, N$, and for each firm we observe prices, inputs, and wages. From these data we can use GMM to back out the values of $\\alpha_1, \\alpha_2$, by applying the analogy principle, which would imply:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{N}\\sum_i \\left(p_{1i}\\alpha_1x_{1i}^{\\alpha_1 - 1} - w_i\\right) &= 0 \\\\ \n",
    "\\frac{1}{N}\\sum_i \\left(p_{2i}\\alpha_2x_{2i}^{\\alpha_2-1} - w_i\\right) &= 0 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Our steps will be:\n",
    "1. Simulate this scenario by proposing a DGP.\n",
    "2. Create data.\n",
    "3. Set up our moment conditions.\n",
    "4. Apply the analogy principle to get from expectations to averages.\n",
    "5. Estimate GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats.distributions as iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: The DGP\n",
    "# note that we need 2N\n",
    "def dgp(true_alphas, N):\n",
    "    alpha1, alpha2 = true_alphas\n",
    "    wage = iid.pareto(5).rvs(size=(N,1))\n",
    "    p_1 = iid.norm(10,2).rvs(size=(N,1))\n",
    "    p_2 = iid.gamma(3,2).rvs(size=(N,1)) + 10\n",
    "    \n",
    "    # create x from these draws, with noise\n",
    "    x_1 = ... + iid.norm(scale=2).rvs(size=(N,1))\n",
    "    x_2 = ... + iid.norm(scale=2).rvs(size=(N,1))\n",
    "    \n",
    "    return (x_1, x_2, p_1, p_2, wage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: create data\n",
    "alpha_true = (0.75, 0.5)\n",
    "data = dgp(alpha_true, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57090534",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(np.c_[data], columns = [\"x1\", \"x2\", \"p1\", \"p2\", \"w\"])\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b74b6f",
   "metadata": {},
   "source": [
    "### Creating moment conditions\n",
    "Our moment conditions will have to be functions of the unknown parameter, since this is... unknown. Here I imitate Ethan's code, with some minor changes. First we set up for each observation _i_, what the moment condition should look like. Since we have 2 moment conditions, I am going to stack these horizontally. What will the dimentions of this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a95b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gj(alphas, data):\n",
    "    # unpack my inputs\n",
    "    alpha1, alpha2 = alphas\n",
    "    x_1, x_2, p_1, p_2, wage = data\n",
    "    # set up moments (separately)\n",
    "    moment1 = ...\n",
    "    moment2 = ...\n",
    "    # stack the moments next to each other\n",
    "    moments = np.c_[moment1, moment2]\n",
    "    \n",
    "    return moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "gj(alpha_true, data)[:5] # these are pretty small, bc the alphas are right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gj((0.1, 0.2), data)[:5] # these are much larger!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf07c37",
   "metadata": {},
   "source": [
    "Now, we appply the analogy principle to these moment conditions - by analogy, the sample mean approximate the expectation, so we want to take the mean over N. In our data, the _N_ are running down the _rows_. The rows are the 0-axis in python, so we want to take the mean over ``axis = 0``. If we apply the mean over ``axis = 1``, this would allow us to take the mean across the columns, which wouldn't make sense here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gN(alphas, data):\n",
    "    # get individual moments\n",
    "    e = gj(alphas, data)\n",
    "    # take mean\n",
    "    gN = e.mean(axis=0).reshape((len(alphas),1))\n",
    "    return gN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ad5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: we want these = 0!\n",
    "print(f\"using true alphas: Eg_j =\\n {gN(alpha_true, data)}\\n\")\n",
    "print(f\"using other alphas: Eg_j =\\n {gN((0.9, 0.9), data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db48e2",
   "metadata": {},
   "source": [
    "Now we have our analogous moment conditions, and GMM will try and find the alphas that minimize the squared errors from these according to:\n",
    "\n",
    "$$E(g_j(a))'\\Omega^{-1} E(g_j(a))$$\n",
    "\n",
    "(a 2 x 2 matrix).\n",
    "\n",
    "But... what is $\\Omega$? The most efficient estimator would weight by the covariance of the moment conditions. So let's set it up!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59261b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invOmega(alphas, data):\n",
    "    e = gj(alphas, data)\n",
    "    # recenter\n",
    "    e = e - e.mean(axis=0)\n",
    "    N = e.shape[0]\n",
    "    var = ...\n",
    "    return np.linalg.inv(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "invOmega(alpha_true, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together, we get the objective function we want to minimize:\n",
    "def J(alphas,omega,data):\n",
    "\n",
    "    g = gN(alphas,data) # Sample moments\n",
    "    N = data[0].shape[0]\n",
    "\n",
    "    return (N*...).squeeze() # Scale by sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eed810",
   "metadata": {},
   "outputs": [],
   "source": [
    "J(alpha_true,invOmega(alpha_true, data),data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ad4aa",
   "metadata": {},
   "source": [
    "### The 2-step estimator.\n",
    "So now we have all the ingredients, we have set up the moment conditions and the objective function to minimize J(). But J() depends on $\\Omega$, which is unknown! The idea now will be to proceed in 2 steps:\n",
    "\n",
    "1. Guess $\\Omega$. We'll guess the identity matrix.\n",
    "2. Minimize J using our guess for $\\Omega$.\n",
    "3. Use our estimated alphas to calculate $\\widehat{\\Omega}$.\n",
    "4. Using $\\widehat{\\Omega}$, re-run GMM to re-estimate the alphas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e294b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Guess Omega.\n",
    "Omega_guess = np.eye(len(alpha_true))\n",
    "\n",
    "# Step 2: Minimize J using our guess. \n",
    "# We're bringing back our old friend, the minimize function!\n",
    "minimizer = minimize(lambda a: ..., x0 = [0.5,0.5])\n",
    "alpha_hat = minimizer.x\n",
    "print(f\"alpha_hat from first stage = {alpha_hat}\")\n",
    "\n",
    "# Step 3: Update Omega\n",
    "invOmega_hat = invOmega(tuple(alpha_hat), data)\n",
    "print(f\"Omega_hat =\\n {invOmega_hat}\")\n",
    "\n",
    "# Step 4: re-run with omega_hat\n",
    "minimizer2 = minimize(lambda a: ..., x0 = [0.5,0.5])\n",
    "alpha_hat2 = minimizer2.x\n",
    "print(f\"alpha_hat from second stage = {alpha_hat2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
