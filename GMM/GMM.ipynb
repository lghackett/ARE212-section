{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9393956",
   "metadata": {},
   "source": [
    "# Introduction to GMM\n",
    "\n",
    "## What is this and why am I learning it?\n",
    "GMM is a way of uncovering structural parameters from data that may or may not be linear. While we have seen over and over again how regressions can uncover unknown \"betas\" that summarize causal (and correlational) relationships between variables, these \"betas\" are _linear_: $y$ is a linear function of $\\beta$. However, in many of the models we may write down (especially in Trade, Economic Geography, IO, etc...) we will have structural relationships that are not at all linear in the unknown parameters we want to estimate! \n",
    "\n",
    "To give a real-world example, take the system of structural equations from the model in Ahfeldt et al (2015) in which they estimate an economic geography model that seeks to explain the changes in rental rates and wages that West Berlin experienced during the Cold War when it was cut off from East Berlin. Their model has neighborhood amenities $\\tilde{a}_{it}$ in a structural residual that capture benefits of living in a neighborhood that aren't related to model mechanisms (commuting, labor supply and demand, rents). This statement in and of itself is a moment condition/assertion: that in the data, the spatial distribution or gradient of changes in these amenities should not be correlated with the distance to the Berlin Wall. That is, if I remove everything the model says is important, leaving me with residuals $\\tilde{a}_{it}$, I shouldn't be able to predict changes in how nice a neighborhood is as a function of the distance to the Wall. Their moment conditions are:\n",
    "\n",
    "$$E(I_k \\times \\Delta \\ln \\tilde{a}_{it}/a_t(\\Lambda)) = 0$$ \n",
    "\n",
    "where $a_t$ is the average amenity in year $t$, and $\\tilde{a}_{it}/a_t$ is a non-linear function of the model's parameter (fundamentals) set $\\Lambda$.\n",
    "\n",
    "**Check understanding:** Try and think about what would happen if you tried to estimate this via OLS. What would go wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaca63b",
   "metadata": {},
   "source": [
    "## A more simple example\n",
    "\n",
    "Let's walk through a more stylized example for our purposes. Suppose you have written down an IO model that goes like this:\n",
    "\n",
    "Firms produce 2 goods, $g\\in\\{1,2\\}$ using a single input $x$ for each. For example, they have one input, corn masa, and can use it to make either tortillas or sopes.  However, each good uses a different technology, like so:\n",
    "\n",
    "$$y_g = f_g(x)$$\n",
    "\n",
    "This is essentially what we saw in class. Now to be more explicit, we are going to say that our model assumes Cobb-Douglas technology, so that our production functions look like this:\n",
    "\n",
    "\\begin{align}\n",
    "y_1 = x_1^{\\alpha_1} \\\\ \n",
    "y_2 = x_2^{\\alpha_2}  \n",
    "\\end{align}\n",
    "\n",
    "$x_1, x_2$ index how much of $x$ is used in the production of goods 1, 2 respectively.\n",
    "\n",
    "Now firms have uncertainty over their prices. Then our first order conditions from the model imply that the following should hold:\n",
    "\\begin{align}\n",
    "E\\left(p_1\\alpha_1x_1^{\\alpha_1 - 1} - w\\right) &= 0 \\\\ \n",
    "E\\left(p_2\\alpha_2x_2^{\\alpha_2-1} - w\\right) &= 0 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Now, we have data on lots of firms $f = 1,\\ldots, N$, and for each firm we observe prices, inputs, and wages. From these data we can use GMM to back out the values of $\\alpha_1, \\alpha_2$, by applying the analogy principle, which would imply:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{N}\\sum_i \\left(p_{1i}\\alpha_1x_{1i}^{\\alpha_1 - 1} - w_i\\right) &= 0 \\\\ \n",
    "\\frac{1}{N}\\sum_i \\left(p_{2i}\\alpha_2x_{2i}^{\\alpha_2-1} - w_i\\right) &= 0 \\\\\n",
    "\\end{align}\n",
    "\n",
    "These are our $g_j$ functions! Intuitively what follows is that we will numerically find a vector $[\\widehat{\\alpha}_1, \\widehat{\\alpha}_2]$ that forces the left-hand side of this equation to be as close to zero as possible- trying to make this equation hold. In this case we have 2 equations and 2 unknowns, so GMM should be able to solve the system exactly. In the case that we had more conditions than unknown parameters, GMM will almost surely _not_ be able to exactly solve all equations, but it will find the parameter set that gets closest to doing so. This is actually a good thing- it provides us with the opportunity to test how well GMM is doing.\n",
    "\n",
    "**Check your understanding:** Why can't we learn anything about the appropriateness of the moment conditions when GMM is just identified? Intuitively, how can we learn about how appropriate the moment conditions are in the over-dentified case?\n",
    "\n",
    "In summary, our steps to reproduce and estimate this model will be:\n",
    "1. Simulate this scenario by proposing a DGP.\n",
    "2. Create data.\n",
    "3. Set up our moment conditions.\n",
    "4. Apply the analogy principle to get from expectations to averages.\n",
    "5. Estimate GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats.distributions as iid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: The DGP\n",
    "# note that we need 2N\n",
    "def dgp(true_alphas, N):\n",
    "    alpha1, alpha2 = true_alphas\n",
    "    wage = iid.pareto(5).rvs(size=(N,1))\n",
    "    p_1 = iid.norm(10,2).rvs(size=(N,1))\n",
    "    p_2 = iid.gamma(3,2).rvs(size=(N,1)) + 10\n",
    "    \n",
    "    # create x from these draws, with noise\n",
    "    x_1 = (wage/(p_1*alpha1))**(1/(alpha1-1)) + iid.norm(scale=2).rvs(size=(N,1))\n",
    "    x_2 = (wage/(p_2*alpha2))**(1/(alpha2-1)) + iid.norm(scale=2).rvs(size=(N,1))\n",
    "    \n",
    "    return (x_1, x_2, p_1, p_2, wage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: create data\n",
    "alpha_true = (0.75, 0.5)\n",
    "data = dgp(alpha_true, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57090534",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(np.concatenate(data, axis=1), columns = [\"x1\", \"x2\", \"p1\", \"p2\", \"w\"])\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b74b6f",
   "metadata": {},
   "source": [
    "### Creating moment conditions\n",
    "Our moment conditions will have to be functions of the unknown parameter, since this is... unknown. Here I imitate Ethan's code, with some minor changes. First we set up for each observation _i_, what the moment condition should look like. Since we have 2 moment conditions, I am going to stack these horizontally. What will the dimentions of this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a95b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gj(alphas, data):\n",
    "    # unpack my inputs\n",
    "    alpha1, alpha2 = alphas\n",
    "    x_1, x_2, p_1, p_2, wage = data\n",
    "    # set up moments (separately)\n",
    "    moment1 = p_1*alpha1*x_1**(alpha1-1) - wage\n",
    "    moment2 = p_2*alpha2*x_2**(alpha2-1) - wage\n",
    "    # stack the moments next to each other\n",
    "    moments = np.concatenate([moment1, moment2], axis=1)\n",
    "    \n",
    "    return moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "gj(alpha_true, data)[:5] # these are pretty small, bc the alphas are right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gj((0.1, 0.1), data)[:5] # these are much larger!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf07c37",
   "metadata": {},
   "source": [
    "Now, we appply the analogy principle to these moment conditions - by analogy, the sample mean approximate the expectation, so we want to take the mean over N. In our data, the _N_ are running down the _rows_. The rows are the 0-axis in python, so we want to take the mean over ``axis = 0``. If we apply the mean over ``axis = 1``, this would allow us to take the mean across the columns, which wouldn't make sense here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gN(alphas, data):\n",
    "    # get individual moments\n",
    "    e = gj(alphas, data)\n",
    "    # take mean\n",
    "    gN = e.mean(axis=0).reshape((len(alphas),1))\n",
    "    return gN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ad5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: we want these = 0!\n",
    "print(f\"using true alphas: Eg_j =\\n {gN(alpha_true, data)}\\n\")\n",
    "print(f\"using other alphas: Eg_j =\\n {gN((0.9, 0.9), data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db48e2",
   "metadata": {},
   "source": [
    "Now we have our analogous moment conditions, and GMM will try and find the alphas that minimize the squared errors from these according to:\n",
    "\n",
    "$$E(g_j(a))'\\Omega^{-1} E(g_j(a))$$\n",
    "\n",
    "(a 2 x 2 matrix).\n",
    "\n",
    "But... what is $\\Omega$? The most efficient estimator would weight by the covariance of the moment conditions. So let's set it up!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59261b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invOmega(alphas, data):\n",
    "    e = gj(alphas, data)\n",
    "    # recenter\n",
    "    e = e - e.mean(axis=0)\n",
    "    N = e.shape[0]\n",
    "    var = e.T@e/N\n",
    "    return np.linalg.inv(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "invOmega(alpha_true, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together, we get the objective function we want to minimize:\n",
    "def J(alphas,invomega,data):\n",
    "\n",
    "    g = gN(alphas,data) # Sample moments\n",
    "    N = data[0].shape[0]\n",
    "\n",
    "    return ((N*g.T)@invomega@g).squeeze() # Scale by sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eed810",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'J at the true alphas = {J(alpha_true,invOmega(alpha_true, data),data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9061d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'J at some wrong alphas = {J(alpha_true,invOmega((0.1, 0.1), data),data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e9408",
   "metadata": {},
   "source": [
    "**Check your understanding:** Why doesn't J = 0 at the true alphas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ad4aa",
   "metadata": {},
   "source": [
    "### The 2-step estimator.\n",
    "So now we have all the ingredients, we have set up the moment conditions and the objective function to minimize J(). But J() depends on $\\Omega$, which is unknown! The idea now will be to proceed in 2 steps:\n",
    "\n",
    "1. Guess $\\Omega$. We'll guess the identity matrix.\n",
    "2. Minimize J using our guess for $\\Omega$.\n",
    "3. Use our estimated alphas to calculate $\\widehat{\\Omega}$.\n",
    "4. Using $\\widehat{\\Omega}$, re-run GMM to re-estimate the alphas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e294b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Guess Omega.\n",
    "Omega_guess = np.eye(len(alpha_true))\n",
    "\n",
    "# Step 2: Minimize J using our guess. \n",
    "# We're bringing back our old friend, the minimize function!\n",
    "minimizer = minimize(lambda a: J(a, Omega_guess, data), x0 = [0.5,0.5], method = 'Nelder-Mead')\n",
    "alpha_hat = minimizer.x\n",
    "print(f'Remember true alphas: {alpha_true}\\n')\n",
    "print(f\"alpha_hat from first stage = {alpha_hat} -> gives J = {minimizer.fun}\\n\")\n",
    "\n",
    "# Step 3: Update Omega\n",
    "invOmega_hat = invOmega(tuple(alpha_hat), data)\n",
    "print(f\"Omega_hat =\\n {invOmega_hat}\\n\")\n",
    "\n",
    "# Step 4: re-run with omega_hat. May as well use our current alpha_hats as new starting point\n",
    "minimizer2 = minimize(lambda a: J(a, invOmega_hat, data), x0 = alpha_hat, method = 'Nelder-Mead')\n",
    "alpha_hat2 = minimizer2.x\n",
    "print(f\"alpha_hat from second step = {alpha_hat2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the thing we're minimizing \n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "# hold alpha_2 at its correct value; vary alpha_1\n",
    "# zooming in for easier viewing\n",
    "alpha1 = np.linspace(0.5, 0.85, 100)\n",
    "j = [J((a1, alpha_hat[1]), invOmega_hat, data) for a1 in alpha1]\n",
    "\n",
    "# zooming out and taking logs to help with viewing\n",
    "alpha1_wide = np.linspace(0.1, 1, 100)\n",
    "j_log = [np.log(J((a1, alpha_hat[1]), invOmega_hat, data)) for a1 in alpha1_wide]\n",
    "\n",
    "ax[0].plot(alpha1, j, label='J function')\n",
    "ax[0].axvline(x=alpha_true[0], color='red', label='True alpha 1')\n",
    "ax[0].legend()\n",
    "ax[0].set(xlabel=\"alpha1 guess\",ylabel=\"J function\")\n",
    "\n",
    "ax[1].plot(alpha1_wide, j_log, label='log J function')\n",
    "ax[1].axvline(x=alpha_true[0], color='red', label='True alpha 1')\n",
    "ax[1].legend()\n",
    "ax[1].set(xlabel=\"alpha1 guess\",ylabel=\"log J function\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
